{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, sklearn, os, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for reading the csv files and dropping the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 3 3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.752</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.89000</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>73.289</td>\n",
       "      <td>4</td>\n",
       "      <td>classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0</td>\n",
       "      <td>-25.427</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.98900</td>\n",
       "      <td>0.897000</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>133.630</td>\n",
       "      <td>4</td>\n",
       "      <td>classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>9</td>\n",
       "      <td>-30.790</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.98700</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>125.610</td>\n",
       "      <td>3</td>\n",
       "      <td>classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>2</td>\n",
       "      <td>-27.272</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.91800</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>79.801</td>\n",
       "      <td>4</td>\n",
       "      <td>classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>2</td>\n",
       "      <td>-16.132</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.74800</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>85.031</td>\n",
       "      <td>4</td>\n",
       "      <td>classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6510</td>\n",
       "      <td>0.7610</td>\n",
       "      <td>10</td>\n",
       "      <td>-7.801</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.44600</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.8690</td>\n",
       "      <td>139.526</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.8710</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.821</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.12100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>141.060</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.6170</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.4360</td>\n",
       "      <td>99.095</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.631</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.23800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>139.920</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.5180</td>\n",
       "      <td>10</td>\n",
       "      <td>-11.607</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.37900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>142.079</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.2750  0.1570    7   -18.752     1       0.0636       0.89000   \n",
       "1         0.2210  0.1260    0   -25.427     1       0.0447       0.98900   \n",
       "2         0.2890  0.0306    9   -30.790     0       0.0446       0.98700   \n",
       "3         0.0753  0.0700    2   -27.272     1       0.0440       0.91800   \n",
       "4         0.1300  0.1580    2   -16.132     1       0.0350       0.74800   \n",
       "..           ...     ...  ...       ...   ...          ...           ...   \n",
       "45        0.6510  0.7610   10    -7.801     1       0.2500       0.44600   \n",
       "46        0.8710  0.6390    7    -7.821     1       0.3490       0.12100   \n",
       "47        0.6170  0.4770    1    -9.889     1       0.3600       0.00422   \n",
       "48        0.8500  0.5640    1    -9.631     0       0.3830       0.23800   \n",
       "49        0.7360  0.5180   10   -11.607     0       0.4680       0.37900   \n",
       "\n",
       "    instrumentalness  liveness  valence    tempo  time_signature    label  \n",
       "0           0.842000    0.1860   0.3040   73.289               4  classic  \n",
       "1           0.897000    0.1020   0.2160  133.630               4  classic  \n",
       "2           0.911000    0.1020   0.1180  125.610               3  classic  \n",
       "3           0.947000    0.1460   0.0625   79.801               4  classic  \n",
       "4           0.924000    0.1000   0.0998   85.031               4  classic  \n",
       "..               ...       ...      ...      ...             ...      ...  \n",
       "45          0.000035    0.1110   0.8690  139.526               4      rap  \n",
       "46          0.000000    0.1930   0.7640  141.060               4      rap  \n",
       "47          0.000000    0.0830   0.4360   99.095               4      rap  \n",
       "48          0.000000    0.1110   0.3480  139.920               4      rap  \n",
       "49          0.000000    0.0989   0.7710  142.079               4      rap  \n",
       "\n",
       "[1382 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['id', 'uri', 'duration_ms']\n",
    "\n",
    "def read_csv_files(file_name, cols_to_drop=cols_to_drop): \n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    labels = df['label']\n",
    "    \n",
    "    # This makes the categorical labels numbers. Print it to see... Better than labels.cat.codes\n",
    "    y = labels.factorize()[0]\n",
    "    \n",
    "    # Add new axis to y in order to be able to transpose it when needed \n",
    "    # y = y[:, np.newaxis]\n",
    "    return df, y\n",
    "\n",
    "df, y = read_csv_files(\"data\\csvs\\dataframeV1.csv\")\n",
    "\n",
    "print(y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier. \n",
    "Let's first get to know the calculations before the actual implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector $ {\\displaystyle \\mathbf {x} =(x_{1},\\ldots ,x_{n})}$ representing some n features (independent variables), it assigns to this instance probabilities\n",
    "$$ {\\LARGE{ \\displaystyle p(C_{k}\\mid x_{1},\\ldots ,x_{n})\\,}}$$\n",
    "for each of K possible outcomes or classes ${\\displaystyle C_{k}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using Bayes' theorem, the conditional probability can be decomposed as\n",
    "$$ {\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}{p(\\mathbf {x} )}}\\,} $$\n",
    "In plain English, using Bayesian probability terminology, the above equation can be written as\n",
    "$$ {\\displaystyle {\\text{posterior}}={\\frac {{\\text{prior}}\\times {\\text{likelihood}}}{\\text{evidence}}}\\,} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on ${\\displaystyle C}$ and the values of the features ${\\displaystyle x_{i}}$ are given, so that the denominator is effectively constant. So the evidence,\n",
    "$ {\\displaystyle \\sum _{k}p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}$\n",
    "is a scaling factor dependent only on ${\\displaystyle x_{1},\\ldots ,x_{n}}$, that is, a constant if the values of the feature variables are known. So we shall drop it since it does not influence the classification.\n",
    "\n",
    "Again, in our case, since we have four classes each of which occuring with equal probability of $\\frac{1}{4}$, we can see that the **prior**, $\\mathbb{P}(\\bold{C}_k)$, is effectively constant just like the evidence above. So, we shall drop it as well.\n",
    "\n",
    "\n",
    "Now we have only the **likelihood**, $p(\\mathbf {x} \\mid C_{k})$ , to work with. The likelihood can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:\n",
    "\n",
    "$$\\displaystyle{p(x_1, x_2, \\ldots, x_n \\mid C_k) = p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2}\\mid x_{3},\\ldots ,x_{n},C_{k})\\cdots p(x_{n-1}\\mid x_{n},C_{k})\\ p(x_{n}\\mid C_{k})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the **naive** conditional independence assumptions come into play: assume that all features in $\\mathbf{x}$ are mutually independent, conditional on the category $\\bold{C_{k}}$. Under this assumption,\n",
    "\n",
    "$$\\displaystyle {p(x_{i}\\mid x_{i+1},\\ldots ,x_{n},C_{k})=p(x_{i}\\mid C_{k})\\,}.$$\n",
    "\n",
    "Thus, the joint model, after dropping the prior and the evidence, can be expressed as **proportional** to $\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Distribution should we use for the joint model **proportional** to $\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})$? Even though there are too many distributions to use here, we opt for Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, $\\bold{x}$. The data is first **segmented** by the class, and then the **mean** and **variance** of $\\bold{x}$ is computed in each class. Let $\\bold{\\mu _{k}}$ be the mean of the values in $\\bold{x}$ associated with class $\\bold{C}_k $, and let $\\bold{\\sigma}_{k}^{2}$ be the unbiased sample variance of the values in $\\bold{x}$ associated with class $\\bold{C}_k $ (that is, the degree of freedom is 1 => n-1). Suppose one has collected some observation value v. Then, the probability density of v given a class $\\bold{C}_k $, $\\displaystyle{p(x=v\\mid C_{k})}$, can be computed by plugging v into the equation for a normal distribution parameterized by $\\displaystyle{\\mu _{k}}$ and $\\displaystyle{\\sigma _{k}^{2}}$ as thus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\LARGE{ \\mathbb{P}(\\bold{X} = x | \\mathbb{C_k}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^{2}}} \\LARGE{e^{- \\frac{(x-\\mu_k)^2}{2\\sigma_{k}^{2}}}} }\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify samples, one has to determine which posterior is greater: classic, jazz, metal or rap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the classification as rap the posterior is given by\n",
    "\n",
    "$$ posterior(rap) = \\frac{\\mathbb{P}(rap)\\mathbb{P}(danceability|rap)...\\mathbb{P}(tempo|rap)\\mathbb{P}(timeSignature|rap)}{evidence} $$\n",
    "where the\n",
    "\n",
    "$$ \\begin{align}\n",
    "evidence &= \\mathbb{P}(classic)\\mathbb{P}(danceability|classic)...\\mathbb{P}(timeSignature|classic) \\\\\n",
    "        &+ \\mathbb{P}(jazz)\\mathbb{P}(danceability|jazz)...\\mathbb{P}(timeSignature|jazz)\\\\\n",
    "        &+ \\mathbb{P}(metal)\\mathbb{P}(danceability|metal)...\\mathbb{P}(timeSignature|metal) \\\\\n",
    "        &+ \\mathbb{P}(rap)\\mathbb{P}(danceability|rap)...\\mathbb{P}(timeSignature|rap) \n",
    "\\end{align}$$\n",
    "and prior = $\\frac{1}{4}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we know that the evidence and the prior are effectively constants, we can drop them as already explained above. Thus:\n",
    "\n",
    "- posterior(classic) = $\\mathbb{P}(danceability|classic) \\dots \\mathbb{P}(tempo|classic)\\mathbb{P}(timeSignature|classic)$\n",
    "- posterior(jazz) = $\\mathbb{P}(danceability|jazz)\\dots \\mathbb{P}(timeSignature|jazz)$\n",
    "- posterior(metal) = $\\mathbb{P}(danceability|metal)\\dots \\mathbb{P}(timeSignature|metal)$ \n",
    "- posterior(rap) = $\\mathbb{P}(danceability|rap)\\dots \\mathbb{P}(tempo|rap)\\mathbb{P}(timeSignature|rap)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We use the log-likelihood since the product of a large number of small probabilities can easily underflow the numerical precision of the computer. And this is resolved by computing instead the sum of the log probabilities as thus:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\large{log\\prod _{i=1}^{N}p(x_{i}\\mid C_{k}) }\n",
    "&=  \\large{ \\sum_{i=1}^N log \\left( \\mathbb{P}(\\bold{X} = x_i | \\mathbb{C_k} \\right)} \\\\\n",
    "&=  \\large{ \\sum_{i=1}^N log \\left(\\frac{1}{\\sqrt{2\\pi\\sigma_{j,k}^{2}}}e^{- \\large{ \\frac{(x_i -\\mu_{j,k})^2}{2\\sigma_{j,k}^{2}}}} \\right) }\\\\\n",
    "&= \\large{ \\sum_{i=1}^N -\\frac{1}{2}log \\left(2\\pi\\sigma_{j,k}^2\\right) - \\frac{(x_i -\\mu_{j,k})^2}{2\\sigma_{j,k}^{2}} }\n",
    "\\end{align}$$\n",
    "\n",
    "Where k is the class label and j is the index of the feature at column j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getiing the MLE for the Mean and the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```LOG-LIKELIHOOD```  =  $\\bold{\\ell}(\\mu_{(j,k)}, \\sigma_{(j,k)}^2) = -\\frac{1}{2}\\sum_{i=1}^N log(2\\pi) -\\sum_{i=1}^N log \\left(\\sigma_{j,k}\\right) -\\frac{1}{2} \\sum_{i=1}^N \\frac{(x_i -\\mu_{j,k})^2}{\\sigma_{j,k}^{2}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```MEAN```:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\frac{\\partial \\ell}{\\partial \\mu_{j,k}} &= -\\frac{1}{2} \\sum_{i=1}^N \\frac{2}{\\sigma_{j,k}^2} (x_i -\\mu_{j,k})(-1) = 0 \\\\\n",
    "&\\implies \\sum_{i=1}^N \\frac{x_i - \\mu_{j,k}}{\\sigma_{j,k}^2} = 0 \\\\\n",
    "&\\implies \\frac{1}{\\sigma_{j,k}^2} \\sum_{i=1}^N x_i   -\\frac{1}{\\sigma_{j,k}^2} \\sum_{i=1}^N \\mu_{j,k} = 0 \\\\\n",
    "&\\implies \\sum_{i=1}^N \\mu_{j,k} = \\sum_{i=1}^N x_i \\\\\n",
    "&\\implies \\mu_{j,k} = \\frac{1}{N}\\sum_{i=1}^N x_i\n",
    "\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```VARIANCE```:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\ell}{\\partial \\sigma_{j,k}^2} &= -\\sum_{i=1}^N \\frac{1}{\\sigma_{j,k}} + \\sum_{i=1}^N \\frac{(x_i - \\mu_{j,k})^2}{\\sigma_{j,k}^3} = 0 \\\\\n",
    "& \\implies \\frac{-N}{\\sigma_{j,k}} + \\frac{1}{\\sigma_{j,k}^3} \\sum_{i=1}^N (x_i - \\mu_{j,k})^2 = 0 \\\\\n",
    "& \\implies \\frac{1}{\\sigma_{j,k}^3} \\sum_{i=1}^N (x_i - \\mu_{j,k})^2 = \\frac{N}{\\sigma_{j,k}} \\\\\n",
    "& \\implies \\sigma_{j,k}^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu_{j,k})^2\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the best estimate for the mean and variance parameters of a Gaussian are simply the empirical estimates of the mean and variance respectively. These means and variances are for each feature w.r.t each class. For example, the feature danceability w.r.t rap has a mean and variance different than danceability w.r.t classic. So for each feature x, it has a mean and variance for each class c.Therefore, the means and variances must be calculated for each feature (in our case, a whopping 12 features for 4 classes). We did this in a simple one-pass by first filtering out the appropriate instances (w.r.t to the class in question) under each feature, then take the mean and variance as can be seen in the implementation below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the fit and predict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussNaiveBayes(): \n",
    "    def __init__(self):\n",
    "        self.mean_classic = None\n",
    "        self.mean_jazz = None\n",
    "        self.mean_rap = None\n",
    "        self.mean_metal = None\n",
    "        \n",
    "        self.variance_classic = None\n",
    "        self.variance_jazz = None\n",
    "        self.variance_rap = None\n",
    "        self.variance_metal = None\n",
    "        \n",
    "    ##############################################################################Ã \n",
    "    def fit(self, X):\n",
    "        # Notice that these are dictionaries, rather than lists\n",
    "        self.mean_classic   = self.__preprocess(X, \"classic\").mean(numeric_only=True)  #mean_classic[\"danceability\"]\n",
    "        self.mean_jazz      = self.__preprocess(X, \"jazz\").mean(numeric_only=True)\n",
    "        self.mean_metal     = self.__preprocess(X, \"metal\").mean(numeric_only=True)\n",
    "        self.mean_rap       = self.__preprocess(X, \"rap\").mean(numeric_only=True)\n",
    "        \n",
    "        #degree of freedom = 1 => (n-1)\n",
    "        self.variance_classic   = self.__preprocess(X, \"classic\").var(numeric_only=True, ddof=1)  \n",
    "        self.variance_jazz      = self.__preprocess(X, \"jazz\").var(numeric_only=True, ddof=1)\n",
    "        self.variance_metal     = self.__preprocess(X, \"metal\").var(numeric_only=True, ddof=1)\n",
    "        self.variance_rap       = self.__preprocess(X, \"rap\").var(numeric_only=True, ddof=1)\n",
    "        \n",
    "    def __preprocess(self, X, labl):\n",
    "        #index of all rows having label labl\n",
    "        t = np.where(X.label == labl) \n",
    "        \n",
    "        #values of those indices\n",
    "        vals = X.iloc[t]     \n",
    "        \n",
    "        #drop the label because...          \n",
    "        return vals.drop( [\"label\"], axis =1)   \n",
    "    \n",
    "    ######################################################################################Ã \n",
    "    def predict(self, X_test):  #here, X_test without labels\n",
    "        predictions = []\n",
    "        if len(X_test) > 1:\n",
    "            for row in X_test.values:\n",
    "                pred = self.__prob_of_feature_given_label(row)\n",
    "                m = np.argmax([pred[\"classic\"], pred[\"jazz\"], pred[\"metal\"], pred[\"rap\"]])\n",
    "                predictions.append(m)\n",
    "                \n",
    "        #for testing single row\n",
    "        elif len(X_test) == 1:\n",
    "            pred = self.__prob_of_feature_given_label(X_test)\n",
    "            m = np.argmax([pred[\"classic\"], pred[\"jazz\"], pred[\"metal\"], pred[\"rap\"]])\n",
    "            predictions.append(m)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "        \n",
    "    def __prob_of_feature_given_label(self, row):\n",
    "        probs = {\"classic\": 0, \"jazz\": 0, \"metal\": 0, \"rap\": 0}\n",
    "        log = math.log #in base e\n",
    "        for i, val in enumerate(row):\n",
    "            probs[\"classic\"] += log( self.__gauss_func(val, self.mean_classic.iloc[i], self.variance_classic.iloc[i]))\n",
    "            probs[\"jazz\"]   +=  log( self.__gauss_func(val, self.mean_jazz.iloc[i], self.variance_jazz.iloc[i]) )\n",
    "            probs[\"metal\"]  +=  log( self.__gauss_func(val, self.mean_metal.iloc[i], self.variance_metal.iloc[i])) \n",
    "            probs[\"rap\"]    +=  log( self.__gauss_func(val, self.mean_rap.iloc[i], self.variance_rap.iloc[i]) )\n",
    "            #print(probs)\n",
    "        return probs\n",
    "        \n",
    "    def __gauss_func(self, val, mu, sigma):\n",
    "        v = (val-mu)**2\n",
    "        s =  2*sigma**2\n",
    "        euler = math.exp(-v/s)                        # e^(-(x-mean)^2 / 2sigma^2)\n",
    "        scale = 1/math.sqrt(2*math.pi*sigma**2)       #1/sqrt(2pi*sigma^2)\n",
    "        \n",
    "        # return a very miniscule amount much closer to 0 than return 0 itself. This is useful for the log\n",
    "        val = 1.21784378e-315 if (scale*euler)==0.0 else (scale*euler)\n",
    "        #print(val)\n",
    "        return val\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8236994219653179\n",
      "(1, 1) (2, 2) (2, 3) (0, 0) (1, 1) (2, 2) (1, 1) (3, 3) (3, 3) (2, 2) (2, 2) (3, 3) (3, 3) (1, 3) (2, 2) (2, 2) (1, 1) (1, 1) (2, 2) (0, 0) (3, 1) (3, 3) (3, 3) (1, 3) (1, 1) (0, 1) (2, 2) (3, 3) (2, 2) (3, 3) (1, 1) (3, 3) (0, 0) (3, 3) (2, 2) (3, 3) (2, 2) (3, 3) (0, 0) (3, 3) (3, 3) (2, 2) (3, 3) (2, 2) (1, 0) (2, 2) (1, 0) (1, 3) (3, 3) (2, 2) (3, 3) (2, 2) (2, 2) (0, 0) (0, 1) (2, 2) (2, 2) (1, 1) (2, 2) (3, 3) (1, 0) (2, 2) (3, 3) (0, 0) (3, 1) (1, 3) (2, 2) (1, 1) (2, 2) (1, 0) (0, 0) (0, 0) (3, 3) (1, 1) (3, 3) (3, 3) (1, 0) (2, 2) (1, 3) (0, 1) (0, 0) (0, 0) (2, 2) (1, 1) (1, 0) (1, 1) (3, 3) (1, 1) (0, 0) (1, 1) (0, 1) (1, 3) (0, 0) (0, 0) (3, 3) (3, 3) (2, 2) (2, 2) (3, 3) (0, 0) (2, 2) (1, 1) (1, 0) (2, 1) (0, 0) (2, 3) (0, 0) (1, 1) (2, 2) (3, 3) (1, 1) (2, 2) (3, 3) (3, 3) (3, 3) (1, 3) (1, 1) (3, 3) (3, 3) (2, 2) (1, 1) (2, 2) (0, 0) (3, 2) (2, 2) (2, 2) (2, 2) (0, 0) (1, 0) (1, 1) (1, 1) (1, 0) (0, 0) (2, 2) (1, 1) (2, 2) (0, 0) (0, 0) (0, 0) (2, 2) (0, 0) (3, 3) (1, 0) (0, 0) (1, 1) (2, 3) (0, 1) (0, 0) (3, 3) (2, 2) (3, 3) (3, 3) (3, 3) (2, 2) (1, 1) (1, 3) (3, 3) (1, 0) (3, 3) (2, 2) (0, 0) (2, 2) (2, 2) (2, 2) (1, 1) (3, 3) (0, 0) (0, 0) (0, 0) (2, 3) (2, 2) (3, 3) (2, 2) (3, 3) (3, 3) (2, 2) (2, 2) (2, 2) (2, 2) (0, 0) (2, 3) (2, 2) (3, 3) (0, 0) (3, 3) (3, 3) (1, 1) (2, 2) (2, 2) (0, 0) (3, 3) (0, 0) (3, 3) (3, 3) (0, 0) (1, 1) (1, 1) (3, 3) (3, 3) (3, 3) (1, 0) (0, 0) (1, 0) (1, 1) (3, 3) (3, 3) (3, 3) (1, 1) (2, 2) (1, 1) (1, 1) (2, 3) (3, 3) (1, 3) (1, 0) (1, 1) (0, 0) (0, 1) (2, 2) (3, 3) (2, 2) (2, 2) (1, 1) (1, 1) (1, 0) (1, 1) (2, 2) (1, 1) (2, 2) (3, 3) (2, 2) (1, 1) (0, 0) (3, 3) (1, 1) (3, 3) (1, 1) (1, 3) (2, 2) (0, 1) (2, 2) (2, 3) (0, 0) (0, 0) (2, 2) (1, 1) (2, 2) (1, 0) (2, 2) (1, 1) (2, 2) (1, 0) (2, 2) (0, 0) (3, 3) (3, 3) (1, 1) (3, 3) (3, 3) (1, 0) (1, 1) (0, 0) (3, 3) (3, 3) (0, 0) (3, 3) (1, 1) (1, 0) (3, 3) (3, 3) (2, 2) (2, 2) (3, 3) (2, 2) (2, 2) (3, 3) (3, 3) (0, 0) (1, 1) (0, 0) (0, 0) (0, 0) (1, 1) (1, 1) (1, 0) (0, 0) (2, 2) (1, 0) (1, 1) (3, 3) (0, 0) (0, 0) (3, 3) (2, 2) (2, 2) (2, 2) (1, 1) (2, 3) (1, 3) (0, 1) (0, 0) (2, 2) (3, 3) (1, 1) (3, 3) (3, 3) (3, 3) (2, 2) (2, 3) (2, 3) (2, 3) (2, 2) (3, 3) (2, 2) (2, 2) (0, 0) (1, 1) (3, 3) (1, 1) (1, 1) (1, 1) (2, 2) (2, 2) (2, 2) (3, 3) (2, 2) (1, 0) (1, 1) (2, 2) (0, 1) (1, 1) (1, 3) (0, 0) (2, 2) (1, 0) (2, 2) (1, 0) (1, 1) (2, 2) (2, 2) (1, 1) (3, 3) (2, 3) (3, 3) (1, 1) (3, 3) "
     ]
    }
   ],
   "source": [
    "# Note that I didn't drop the label for the df\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, stratify=df[\"label\"], shuffle=True, random_state=42)\n",
    "\n",
    "# remove the label for X_test, not X_train\n",
    "X_test = X_test.drop([\"label\"], axis=1)\n",
    "\n",
    "GNB = GaussNaiveBayes()\n",
    "GNB.fit(X_train)\n",
    "\n",
    "y_pred = GNB.predict(X_test)\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "for i in list( zip(y_test, y_pred)):\n",
    "    print(i, end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1c34a4705f1ad3e989dda6439809041f8cc2de32377149232f1be5f4f19a59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
