{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of gaussian "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  mixture model (GMM) is a parametric probability density function rappresented as a weighted sum of its k components.\n",
    "\n",
    "\n",
    "$$\n",
    "p(x | \\Theta) = \\sum_{k=1}^{K} \\pi_{k} \\mathcal{f}(x | \\Theta ) \\\\\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "0 \\le \\pi_k \\le 1 , \\; \\forall \\; k = 1, ..., K \\\\\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "\\sum_{k=1}^{K} \\pi_k = 1\n",
    "$$\n",
    "\n",
    "In this way we obtain a distribution made up of K  components. The parameter $\\pi_k$ is called the mixing coefficent and rappresent how much of the rispective component is in the resulting distribution.\n",
    "\n",
    "The complete set of parameters for the mixture model with K components is : \n",
    " $$ \n",
    "\\Theta = \\{\\pi_1, ..., \\pi_k, \\theta_1, ...., \\theta_k \\} \\; \n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume to have J subgroups in our dataset $D$, each one being **Heterogenous across** and **homogeneous within**, then each subgroup will have the same parametric family differing  by the values of the parameters across the groups, then by estimating $\\Theta$ we will be able to cluster unobserved realizations of $\\vec{X} = \\{X_1, ...., X_n\\}$ of which $D = \\{x_1, ..., x_n\\} $ is a specific realization.\n",
    "\n",
    "In this instance we assume that we are dealing with multivariate gaussian distributions. hence: \n",
    "\n",
    "$$ \n",
    "p(x | \\Theta) = \\sum_{k=1}^{K} \\pi_{k} \\mathcal{N}(x | \\Theta ) \\\\\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "\\Theta = \\{\\pi_1, ..., \\pi_k, \\mu_1,...,\\mu_k,\\Sigma_1,...,\\Sigma_k \\}\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "p_k(x|\\theta_k) = \\frac{1}{(2\\pi)^{d/2} \\sqrt{|\\Sigma_k|} } e^{-\\frac{1}{2}(x-\\mu_k)^{t}\\Sigma^{-1}_{k}(x-\\mu_k)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to the question on how to estimate the parameters, trying to estimate the parameters is NP given that the Log-likelihood is not jointly concave w.r.t- the mixture parameters. \\\\ \n",
    "Luckily we can use the Expectation Maximization Algorithm for finding the maximum likelihood estimates of a mixture (of gaussians in our case) model. As the name suggest the algorithm is divided in two steps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A dataset $D$ described as above where $\\forall x \\in D, \\;\\; x \\in R^d$.\n",
    "* Independence \n",
    "\n",
    "Moreover we define the membership weight $w_{i,j} = \\frac{p_k(x_i|\\theta_k) \\pi_k}{\\sum_{m=1}^{K} p_m(x_i|\\theta_m) \\pi_m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the membership weights for each datapoint $x_i$,  $1 \\le i \\le N$ and all mixture components $1 \\le k \\le K$. This will yield a N x K matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949ca02c6f6c8e46594f49d827909bb6cc1f655ba57a65a49f724deda5e66916"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
