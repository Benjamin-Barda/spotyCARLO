{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with trees and forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree (DT) is a *non-parametric* supervised method used for both regression and classification. As the name suggests, DTs uses a tree-like model where each internal node rappresent a test on one (or more) attributes of our dataset, and each leaf node rappresent a class label; It follows that the branches rappresent the outcome of our test. The path from root to leaf are the classifications rules.\n",
    "\n",
    "![A simple rappresentation of a decision tree](imgs/simpleDT.jpg#center)\n",
    "\n",
    "\\\n",
    "Decision Trees have many advantages such as : \n",
    "* Simple to understand and even visualize\n",
    "* It is a *white box* model; Non parametric approach that is no assumption on the shape/distribution of the data\n",
    "* Can work with both numerical and categorical values\n",
    "* Easy train and fast performances\n",
    "\n",
    "\n",
    "\n",
    "On the other hand decision trees can be very sensitive to small change in the data that can result in major change in the structure of the tree; Another problem with this approach is the danger of overfitting if not taken enough precautions. \n",
    "\n",
    "\n",
    "\n",
    "**So the question is how can we can construct a decision tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The building process: Reducing impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to build a decision is a 'greedy' algorithm that at each node try to find the variable that **best split** the set of items in each step. So the question becomes what is considered the best split ? \n",
    "\n",
    "Two main metrics are used nowdays : \n",
    "* Gini impurity \n",
    "* Information Gain / Entropy impurity\n",
    "\n",
    "In this brief project we decided to use the latter.\n",
    "\n",
    "In general we define the information gain as:   \n",
    "$$IG(T, \\alpha ) = H(T) - H(T| \\alpha )$$\n",
    "where $$H(X) = - \\sum^n_{i=1} P(x_i)logP(x_i)$$\n",
    "is the entropy. \n",
    "\n",
    "So when building the decision tree we are trying to reduce the conditional entropy that is equivalent to maximise the information gain on each split. In simple terms we are trying to learn $\\alpha$ such that our uncertainty about our observations is minimized. \n",
    "\n",
    "So the recursive process will be to begin with the whole dataset in the root node. We then calculate which is the best split among all features and all possible values for that feature; This will leave us with a threshold and feature on where to split. We assign to the left child of the root all observations where it's value is less or equal than the threshold and the rest to the right child. The process in then recursively repeated untill we either have all obs belonging to one class or where there are not enough observations to further split. In both cases those nodes will be the leaf nodes.\n",
    "\n",
    "\n",
    "For a more Detailed implentation refer to the **DecisionTree\\tree.py** python class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>7</td>\n",
       "      <td>-18.752</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>73.289</td>\n",
       "      <td>152280</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0</td>\n",
       "      <td>-25.427</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>133.630</td>\n",
       "      <td>139307</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>9</td>\n",
       "      <td>-30.790</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0446</td>\n",
       "      <td>0.987</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>125.610</td>\n",
       "      <td>212067</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>2</td>\n",
       "      <td>-27.272</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>79.801</td>\n",
       "      <td>365147</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>2</td>\n",
       "      <td>-16.132</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>85.031</td>\n",
       "      <td>302093</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0        0.2750  0.1570    7   -18.752     1       0.0636         0.890   \n",
       "1        0.2210  0.1260    0   -25.427     1       0.0447         0.989   \n",
       "2        0.2890  0.0306    9   -30.790     0       0.0446         0.987   \n",
       "3        0.0753  0.0700    2   -27.272     1       0.0440         0.918   \n",
       "4        0.1300  0.1580    2   -16.132     1       0.0350         0.748   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_ms  time_signature  \\\n",
       "0             0.842     0.186   0.3040   73.289       152280               4   \n",
       "1             0.897     0.102   0.2160  133.630       139307               4   \n",
       "2             0.911     0.102   0.1180  125.610       212067               3   \n",
       "3             0.947     0.146   0.0625   79.801       365147               4   \n",
       "4             0.924     0.100   0.0998   85.031       302093               4   \n",
       "\n",
       "   Y  \n",
       "0  0  \n",
       "1  0  \n",
       "2  0  \n",
       "3  0  \n",
       "4  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dF = pd.read_csv('data//csvs//dataframeV1.csv', index_col=0)\n",
    "dF = dF.drop(['id', 'uri'], axis = 1)\n",
    "dF.label = pd.Categorical(dF.label)\n",
    "dF['Y'] = dF.label.cat.codes\n",
    "dF = dF.drop(['label'], axis = 1)\n",
    "dF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DecisionTree.tree import Tree\n",
    "\n",
    "# Creating the decision Tree\n",
    "dT = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for the decision tree\n",
    "y = dF.Y \n",
    "X = dF.drop([\"Y\"], axis=1)\n",
    "\n",
    "# Split in train and validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=43, train_size=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tree on the train set \n",
    "dT.fit(x_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prediction over the validation set\n",
    "preds = dT.predict(x_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8988439306358381"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the overall accuracy.\n",
    "sum(preds == y_test) / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to address one of the main problems when using DTs, that is overfitting. In fact is common for such models to learn on the noise and small variations in the data, if hyper-paramethers are not carefully tuned. \n",
    "\n",
    "A solution to this is to use Random Forests (RF). RF is an **ensamble learning** method that operates by constructing multiple decision trees. Many decision trees are constructed applying the general technique of bootstrapping.\n",
    "\n",
    "More formally if we have a training set $X = x_1, ..., x_n$ and the labels associated $Y = y_1, ...,y_n$ we **Bag** (Sample with replacment) from $\\{X,Y\\}$ $ B $ times and train a decision tree on this sample. We then in case of classification take the majority of votes from all the $B$ predictions as the final output of the RF model.\n",
    "\n",
    "Bootstrapping so help us solve the issue of overfitting. A step further in this direction would be modify the DT algorithm slightly, by looking at each step to only a portion of the features to find the best possible split. This not only improve the overall model accuracy and speed but also provide a further guard against overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomForest.randomForest import Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first question is how many trees should build ? As there is no universal answer the best way to find out this hyperparameter is to use some model selection techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'numpy.ndarray' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\decisionTree.ipynb Cella 19\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benjamin.barda/Documents/GitHub/spotyCARLO/decisionTree.ipynb#ch0000033?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models : \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benjamin.barda/Documents/GitHub/spotyCARLO/decisionTree.ipynb#ch0000033?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mfit(x_train, y_train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/benjamin.barda/Documents/GitHub/spotyCARLO/decisionTree.ipynb#ch0000033?line=4'>5</a>\u001b[0m     score[model\u001b[39m.\u001b[39mget_max_trees] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mscore(x_test, y_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/benjamin.barda/Documents/GitHub/spotyCARLO/decisionTree.ipynb#ch0000033?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mget_max_trees()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\RandomForest\\randomForest.py:54\u001b[0m, in \u001b[0;36mForest.score\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, X, y): \n\u001b[1;32m---> 54\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(preds \u001b[39m==\u001b[39m y) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(preds)\n",
      "File \u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\RandomForest\\randomForest.py:43\u001b[0m, in \u001b[0;36mForest.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     40\u001b[0m preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mB, \u001b[39mlen\u001b[39m(X)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64)\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m b,tree \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforest) :\n\u001b[1;32m---> 43\u001b[0m     pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tree\u001b[39m.\u001b[39;49mpredict(X), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint8)\n\u001b[0;32m     44\u001b[0m     preds[b, :] \u001b[39m=\u001b[39m pred\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)) : \n",
      "File \u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\DecisionTree\\tree.py:196\u001b[0m, in \u001b[0;36mTree.predict\u001b[1;34m(self, X, trgt)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X, trgt \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) :\n\u001b[0;32m    188\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m    Bulk prediction \u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m        {list} predictions : list of predictions on X\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m \n\u001b[1;32m--> 196\u001b[0m     predictions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traverse(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m trgt \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m : \n\u001b[0;32m    199\u001b[0m         \u001b[39massert\u001b[39;00m trgt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions) \n",
      "File \u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\DecisionTree\\tree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X, trgt \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) :\n\u001b[0;32m    188\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39m    Bulk prediction \u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m        {list} predictions : list of predictions on X\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m \n\u001b[1;32m--> 196\u001b[0m     predictions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_traverse(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[0;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m trgt \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m : \n\u001b[0;32m    199\u001b[0m         \u001b[39massert\u001b[39;00m trgt\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions) \n",
      "File \u001b[1;32mc:\\Users\\benjamin.barda\\Documents\\GitHub\\spotyCARLO\\DecisionTree\\tree.py:183\u001b[0m, in \u001b[0;36mTree._traverse\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mis_leaf():\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m node\u001b[39m.\u001b[39mvalue\n\u001b[1;32m--> 183\u001b[0m \u001b[39mif\u001b[39;00m x[node\u001b[39m.\u001b[39;49mfeature] \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m node\u001b[39m.\u001b[39;49mthresh:\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traverse(x, node\u001b[39m.\u001b[39mleft)\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traverse(x, node\u001b[39m.\u001b[39mright)\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'numpy.ndarray' and 'str'"
     ]
    }
   ],
   "source": [
    "models = [Forest(max_trees=x) for x in range(3,32+1)]\n",
    "score = {}\n",
    "for model in models : \n",
    "    model.fit(x_train.to_numpy(), y_train.to_numpy())\n",
    "    score[model.get_max_trees] = model.score(x_test.to_numpy(), y_test.to_numpy())\n",
    "    print(f\"{model.get_max_trees()}\", end = '\\r')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949ca02c6f6c8e46594f49d827909bb6cc1f655ba57a65a49f724deda5e66916"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
