{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with trees and forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree (DT) is a *non-parametric* supervised method used for both regression and classification. As the name suggests, DTs uses a tree-like model where each internal node rappresent a test on one (or more) attributes of our dataset, and each leaf node rappresent a class label; It follows that the branches rappresent the outcome of our test. The path from root to leaf are the classifications rules.\n",
    "\n",
    "![A simple rappresentation of a decision tree](imgs/simpleDT.jpg#center)\n",
    "\n",
    "\\\n",
    "Decision Trees have many advantages such as : \n",
    "* Simple to understand and even visualize\n",
    "* It is a *white box* model; Non parametric approach that is no assumption on the shape/distribution of the data\n",
    "* Can work with both numerical and categorical values\n",
    "* Easy train and fast performances\n",
    "\n",
    "\n",
    "\n",
    "On the other hand decision trees can be very sensitive to small change in the data that can result in major change in the structure of the tree; Another problem with this approach is the danger of overfitting if not taken enough precautions. \n",
    "\n",
    "\n",
    "\n",
    "**So the question is how can we can construct a decision tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The building process: Reducing impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to build a decision is a 'greedy' algorithm that at each node try to find the variable that **best split** the set of items in each step. So the question becomes what is considered the best split ? \n",
    "\n",
    "Two main metrics are used nowdays : \n",
    "* Gini impurity \n",
    "* Information Gain / Entropy impurity\n",
    "\n",
    "In this brief project we decided to use the latter.\n",
    "\n",
    "In general we define the information gain as:   \n",
    "$$IG(T, \\alpha ) = H(T) - H(T| \\alpha )$$\n",
    "where $$H(X) = - \\sum^n_{i=1} P(x_i)logP(x_i)$$\n",
    "is the entropy. \n",
    "\n",
    "So when building the decision tree we are trying to reduce the conditional entropy that is equivalent to maximise the information gain on each split. In simple terms we are trying to learn $\\alpha$ such that our uncertainty about our observations is minimized. \n",
    "\n",
    "So the recursive process will be to begin with the whole dataset in the root node. We then calculate which is the best split among all features and all possible values for that feature; This will leave us with a threshold and feature on where to split. We assign to the left child of the root all observations where it's value is less or equal than the threshold and the rest to the right child. The process in then recursively repeated untill we either have all obs belonging to one class or where there are not enough observations to further split. In both cases those nodes will be the leaf nodes.\n",
    "\n",
    "\n",
    "For a more Detailed implentation refer to the **DecisionTree\\tree.py** python class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dF = pd.read_csv('data//csvs//dataframeV1.csv', index_col=0)\n",
    "\n",
    "# Drop not usefull cols\n",
    "dF = dF.drop(['id', 'uri'], axis = 1)\n",
    "\n",
    "# Turn genres names into cat \n",
    "dF.label = pd.Categorical(dF.label)\n",
    "dF['Y'] = dF.label.cat.codes\n",
    "dF = dF.drop(['label'], axis = 1)\n",
    "\n",
    "# Shuffling the dataset\n",
    "dF = dF.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "dF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DecisionTree.tree import Tree\n",
    "\n",
    "# Creating the decision Tree\n",
    "dT = Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for the decision tree\n",
    "\n",
    "y = dF.Y \n",
    "X = dF.drop([\"Y\"], axis=1)\n",
    "\n",
    "# Split in train and validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=43, train_size=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tree on the train set \n",
    "dT.fit(x_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prediction over the validation set\n",
    "preds = dT.predict(x_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overall accuracy.\n",
    "sum(preds == y_test) / len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to address one of the main problems when using DTs, that is overfitting. In fact is common for such models to learn on the noise and small variations in the data, if hyper-paramethers are not carefully tuned. \n",
    "\n",
    "A solution to this is to use Random Forests (RF). RF is an **ensamble learning** method that operates by constructing multiple decision trees. Many decision trees are constructed applying the general technique of bootstrapping.\n",
    "\n",
    "More formally if we have a training set $X = x_1, ..., x_n$ and the labels associated $Y = y_1, ...,y_n$ we **Bag** (Sample with replacment) from $\\{X,Y\\}$ $ B $ times and train a decision tree on this sample. We then in case of classification take the majority of votes from all the $B$ predictions as the final output of the RF model.\n",
    "\n",
    "Bootstrapping so help us solve the issue of overfitting. A step further in this direction would be modify the DT algorithm slightly, by looking at each step to only a portion of the features to find the best possible split. This not only improve the overall model accuracy and speed but also provide a further guard against overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomForest.randomForest import Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first question is how many trees should build ? As there is no universal answer the best way to find out this hyperparameter is to use some model selection techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_trees' : [x for x in range(24,27)],\n",
    "    'max_detph' : [4,5,6,7]\n",
    "}\n",
    "\n",
    "K = 10\n",
    "\n",
    "\n",
    "res_acc = {}\n",
    "res_time = {}\n",
    "\n",
    "# Set to true if need to rerun the cross validation (note: it takes some time)\n",
    "doCV = False\n",
    "\n",
    "if doCV : \n",
    "    for b in parameters['max_trees']: \n",
    "        res_acc['T' + str(b)] = list()\n",
    "        res_time['T' + str(b)] = list()\n",
    "        for d in parameters['max_detph']:\n",
    "            print(f\"Fitting Forest ID : T{b}D{d}\", end = '\\r')\n",
    "            score = cross_validate(Forest(max_trees=b, max_depth=6), X, y, scoring='accuracy', cv = K, n_jobs=-1 )\n",
    "            res_acc['T' + str(b)].append(sum(score['test_score']) / K)\n",
    "            res_time['T' + str(b)].append(sum(score['score_time']) / K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doCV : \n",
    "    acc_df = pd.DataFrame.from_dict(res_acc, orient='columns')\n",
    "    acc_df = pd.concat([pd.Series(parameters['max_detph'], name = 'Depth'),acc_df], axis = 1)\n",
    "    acc_df = acc_df.set_index('Depth')\n",
    "\n",
    "    time_df = pd.DataFrame.from_dict(res_time, orient='columns')\n",
    "    time_df = pd.concat([pd.Series(parameters['max_detph'], name = 'Depth'),time_df], axis = 1)\n",
    "    time_df = time_df.set_index('Depth')\n",
    "else : \n",
    "    acc_df = pd.read_csv('cv_acc_res.csv')\n",
    "    acc_df = acc_df.set_index('Depth')\n",
    "    time_df = pd.read_csv('cv_time_res.csv')\n",
    "    time_df = time_df.set_index('Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "save = False\n",
    "\n",
    "\n",
    "if save:\n",
    "    with open('cv_acc.csv', 'w') as f: \n",
    "        acc_df.to_csv(f)\n",
    "        f.close()\n",
    "    with open('cv_time.csv', 'w') as f: \n",
    "        time_df.to_csv(f)\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "\n",
    "acc_df.plot(ax = f.gca(), use_index = True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"Accuracy of CV models\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "\n",
    "time_df.plot(ax = f.gca(), use_index = True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"Time to fit\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Time(s)\") \n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "379ef13cdaaa3488d0d0ee4a3d00b9f61dd7f8c251b8a40c38fb20881b24658a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
