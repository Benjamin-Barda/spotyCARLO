{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for reading the csv files and dropping the unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.547</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.878</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.63600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.265</td>\n",
       "      <td>92.151</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.647</td>\n",
       "      <td>0.612</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.08710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.615</td>\n",
       "      <td>145.055</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.624</td>\n",
       "      <td>5</td>\n",
       "      <td>-7.485</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.00919</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.695</td>\n",
       "      <td>160.084</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.528</td>\n",
       "      <td>0.705</td>\n",
       "      <td>4</td>\n",
       "      <td>-7.300</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1560</td>\n",
       "      <td>0.28000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.485</td>\n",
       "      <td>141.280</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.145</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.3270</td>\n",
       "      <td>0.710</td>\n",
       "      <td>137.941</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.761</td>\n",
       "      <td>10</td>\n",
       "      <td>-7.801</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.44600</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.869</td>\n",
       "      <td>139.526</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.871</td>\n",
       "      <td>0.639</td>\n",
       "      <td>7</td>\n",
       "      <td>-7.821</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3490</td>\n",
       "      <td>0.12100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.764</td>\n",
       "      <td>141.060</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.477</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3600</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.436</td>\n",
       "      <td>99.095</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.631</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.23800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.348</td>\n",
       "      <td>139.920</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.736</td>\n",
       "      <td>0.518</td>\n",
       "      <td>10</td>\n",
       "      <td>-11.607</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.37900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.771</td>\n",
       "      <td>142.079</td>\n",
       "      <td>4</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "40         0.547   0.568    0    -8.878     0       0.1970       0.63600   \n",
       "41         0.647   0.612    4    -7.442     0       0.0681       0.08710   \n",
       "42         0.824   0.624    5    -7.485     0       0.0793       0.00919   \n",
       "43         0.528   0.705    4    -7.300     0       0.1560       0.28000   \n",
       "44         0.846   0.546    0   -10.145     1       0.3300       0.20500   \n",
       "45         0.651   0.761   10    -7.801     1       0.2500       0.44600   \n",
       "46         0.871   0.639    7    -7.821     1       0.3490       0.12100   \n",
       "47         0.617   0.477    1    -9.889     1       0.3600       0.00422   \n",
       "48         0.850   0.564    1    -9.631     0       0.3830       0.23800   \n",
       "49         0.736   0.518   10   -11.607     0       0.4680       0.37900   \n",
       "\n",
       "    instrumentalness  liveness  valence    tempo  time_signature label  \n",
       "40          0.000000    0.2430    0.265   92.151               4   rap  \n",
       "41          0.000000    0.2940    0.615  145.055               4   rap  \n",
       "42          0.014700    0.1070    0.695  160.084               4   rap  \n",
       "43          0.000000    0.0876    0.485  141.280               4   rap  \n",
       "44          0.000004    0.3270    0.710  137.941               4   rap  \n",
       "45          0.000035    0.1110    0.869  139.526               4   rap  \n",
       "46          0.000000    0.1930    0.764  141.060               4   rap  \n",
       "47          0.000000    0.0830    0.436   99.095               4   rap  \n",
       "48          0.000000    0.1110    0.348  139.920               4   rap  \n",
       "49          0.000000    0.0989    0.771  142.079               4   rap  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['id', 'uri', 'duration_ms']\n",
    "\n",
    "def read_csv_files(file_name, cols_to_drop=cols_to_drop): \n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    labels = df['label']\n",
    "    \n",
    "    # This makes the categorical labels numbers. Print it to see...\n",
    "    y = labels.factorize()[0]\n",
    "    \n",
    "    return df, y\n",
    "\n",
    "df, y = read_csv_files(\"data\\csvs\\dataframeV1.csv\")\n",
    "\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data\n",
    "\n",
    "We can drop the labels of our training data and then convert them on a single scale. We can standardize the values using the below formula.\n",
    "\n",
    "$$x_i = \\frac{{x}_i - mean(x)} {\\sigma(x)} $$\n",
    "\n",
    "It’s recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.100925</td>\n",
       "      <td>-1.050292</td>\n",
       "      <td>0.511531</td>\n",
       "      <td>-0.834979</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>-0.373430</td>\n",
       "      <td>1.090495</td>\n",
       "      <td>1.715003</td>\n",
       "      <td>0.113724</td>\n",
       "      <td>-0.370043</td>\n",
       "      <td>-1.362347</td>\n",
       "      <td>0.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.368146</td>\n",
       "      <td>-1.141622</td>\n",
       "      <td>-1.470268</td>\n",
       "      <td>-1.661467</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>-0.563558</td>\n",
       "      <td>1.329736</td>\n",
       "      <td>1.869971</td>\n",
       "      <td>-0.530812</td>\n",
       "      <td>-0.735956</td>\n",
       "      <td>0.579970</td>\n",
       "      <td>0.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.031645</td>\n",
       "      <td>-1.422685</td>\n",
       "      <td>1.077759</td>\n",
       "      <td>-2.325506</td>\n",
       "      <td>-1.106597</td>\n",
       "      <td>-0.564564</td>\n",
       "      <td>1.324903</td>\n",
       "      <td>1.909417</td>\n",
       "      <td>-0.530812</td>\n",
       "      <td>-1.143451</td>\n",
       "      <td>0.321814</td>\n",
       "      <td>-1.850984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.089151</td>\n",
       "      <td>-1.306607</td>\n",
       "      <td>-0.904040</td>\n",
       "      <td>-1.889913</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>-0.570600</td>\n",
       "      <td>1.158159</td>\n",
       "      <td>2.010851</td>\n",
       "      <td>-0.193198</td>\n",
       "      <td>-1.374226</td>\n",
       "      <td>-1.152733</td>\n",
       "      <td>0.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.818465</td>\n",
       "      <td>-1.047346</td>\n",
       "      <td>-0.904040</td>\n",
       "      <td>-0.510575</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>-0.661137</td>\n",
       "      <td>0.747340</td>\n",
       "      <td>1.946046</td>\n",
       "      <td>-0.546158</td>\n",
       "      <td>-1.219128</td>\n",
       "      <td>-0.984384</td>\n",
       "      <td>0.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.997108</td>\n",
       "      <td>-1.413846</td>\n",
       "      <td>-0.904040</td>\n",
       "      <td>-1.489855</td>\n",
       "      <td>-1.106597</td>\n",
       "      <td>-0.403609</td>\n",
       "      <td>1.179908</td>\n",
       "      <td>1.681192</td>\n",
       "      <td>-0.581454</td>\n",
       "      <td>-1.419549</td>\n",
       "      <td>-1.553228</td>\n",
       "      <td>-1.850984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.601121</td>\n",
       "      <td>-1.495779</td>\n",
       "      <td>-0.054697</td>\n",
       "      <td>-2.427656</td>\n",
       "      <td>-1.106597</td>\n",
       "      <td>-0.433788</td>\n",
       "      <td>1.346652</td>\n",
       "      <td>2.002398</td>\n",
       "      <td>-0.636700</td>\n",
       "      <td>-1.480673</td>\n",
       "      <td>-1.409408</td>\n",
       "      <td>0.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.625471</td>\n",
       "      <td>-1.465404</td>\n",
       "      <td>1.077759</td>\n",
       "      <td>-2.355718</td>\n",
       "      <td>-1.106597</td>\n",
       "      <td>-0.568588</td>\n",
       "      <td>1.322486</td>\n",
       "      <td>1.765720</td>\n",
       "      <td>-0.695783</td>\n",
       "      <td>-1.395848</td>\n",
       "      <td>0.403349</td>\n",
       "      <td>-1.850984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.804011</td>\n",
       "      <td>-1.498077</td>\n",
       "      <td>-1.187154</td>\n",
       "      <td>-2.644462</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>-0.559534</td>\n",
       "      <td>1.339402</td>\n",
       "      <td>1.931958</td>\n",
       "      <td>-0.839269</td>\n",
       "      <td>-1.474852</td>\n",
       "      <td>0.530238</td>\n",
       "      <td>-1.850984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.447323</td>\n",
       "      <td>-1.310731</td>\n",
       "      <td>-0.337811</td>\n",
       "      <td>-1.412345</td>\n",
       "      <td>-1.106597</td>\n",
       "      <td>-0.548469</td>\n",
       "      <td>1.049413</td>\n",
       "      <td>0.475261</td>\n",
       "      <td>-0.615983</td>\n",
       "      <td>-1.484831</td>\n",
       "      <td>-0.799362</td>\n",
       "      <td>-6.058321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability    energy       key  loudness      mode  speechiness  \\\n",
       "0     -1.100925 -1.050292  0.511531 -0.834979  0.903018    -0.373430   \n",
       "1     -1.368146 -1.141622 -1.470268 -1.661467  0.903018    -0.563558   \n",
       "2     -1.031645 -1.422685  1.077759 -2.325506 -1.106597    -0.564564   \n",
       "3     -2.089151 -1.306607 -0.904040 -1.889913  0.903018    -0.570600   \n",
       "4     -1.818465 -1.047346 -0.904040 -0.510575  0.903018    -0.661137   \n",
       "5     -1.997108 -1.413846 -0.904040 -1.489855 -1.106597    -0.403609   \n",
       "6     -0.601121 -1.495779 -0.054697 -2.427656 -1.106597    -0.433788   \n",
       "7     -1.625471 -1.465404  1.077759 -2.355718 -1.106597    -0.568588   \n",
       "8     -0.804011 -1.498077 -1.187154 -2.644462  0.903018    -0.559534   \n",
       "9     -1.447323 -1.310731 -0.337811 -1.412345 -1.106597    -0.548469   \n",
       "\n",
       "   acousticness  instrumentalness  liveness   valence     tempo  \\\n",
       "0      1.090495          1.715003  0.113724 -0.370043 -1.362347   \n",
       "1      1.329736          1.869971 -0.530812 -0.735956  0.579970   \n",
       "2      1.324903          1.909417 -0.530812 -1.143451  0.321814   \n",
       "3      1.158159          2.010851 -0.193198 -1.374226 -1.152733   \n",
       "4      0.747340          1.946046 -0.546158 -1.219128 -0.984384   \n",
       "5      1.179908          1.681192 -0.581454 -1.419549 -1.553228   \n",
       "6      1.346652          2.002398 -0.636700 -1.480673 -1.409408   \n",
       "7      1.322486          1.765720 -0.695783 -1.395848  0.403349   \n",
       "8      1.339402          1.931958 -0.839269 -1.474852  0.530238   \n",
       "9      1.049413          0.475261 -0.615983 -1.484831 -0.799362   \n",
       "\n",
       "   time_signature  \n",
       "0        0.252684  \n",
       "1        0.252684  \n",
       "2       -1.850984  \n",
       "3        0.252684  \n",
       "4        0.252684  \n",
       "5       -1.850984  \n",
       "6        0.252684  \n",
       "7       -1.850984  \n",
       "8       -1.850984  \n",
       "9       -6.058321  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop([\"label\"], axis=1)\n",
    "\n",
    "def standardise(X):\n",
    "    # mean and standard deviation of all the features irrespective of class differences\n",
    "    mean_all   = X.mean()  \n",
    "    std_all    = X.std() \n",
    "\n",
    "    # standardised values\n",
    "    X_standardised = (X-mean_all)/std_all\n",
    "    return X_standardised\n",
    "\n",
    "X_standardised = standardise(X)\n",
    "X_standardised.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Confusion Matrix and print the Classification Report and Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Classic\", \"Jazz\", \"Metal\", \"Rap\"]\n",
    "\n",
    "def confusion_matrix_score(ytest, ypred, labels=labels):\n",
    "    \n",
    "    score = accuracy_score(ytest, ypred) * 100\n",
    "    print(f\"The accuracy score is {score:.2f}%.\", end=\"\\n\\n\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(ytest, ypred)\n",
    "\n",
    "    sns.heatmap(conf_matrix, square=True, annot=True, fmt='d', cbar=True, lw=0.6,\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "\n",
    "    plt.title(\"Confusion Matrix\", color=\"blue\", fontsize=25)\n",
    "\n",
    "    plt.xlabel('True Label')\n",
    "    plt.ylabel('Predicted Label');\n",
    "\n",
    "    # classification_report\n",
    "    print(f\"classification_report: \\n{ classification_report(ytest, ypred, target_names=labels) }\", end=\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of k-means and k-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is used in market segmentation; where we try to find customers that are similar to each other whether in terms of behaviors or attributes, image segmentation/compression; where we try to group similar regions together, document clustering based on topics, etc. In our case, clustering based on music genre of either being \"Classic\", \"Jazz\", \"Metal\" and \"Rap\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Algorithm\n",
    "Kmeans algorithm is an iterative algorithm that tries to partition the dataset into \"K\" pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\n",
    "\n",
    "That is, K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion:\n",
    "\n",
    "$$\\sum_{i=0}^n \\underset{\\mu_j \\in \\mathbf{C}}{\\text{min}}(||x_i - \\mu_j||^2)$$\n",
    "Inertia can be recognized as a measure of how internally coherent clusters are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way kmeans algorithm works is as follows:\n",
    "\n",
    "1. Specify number of clusters K.\n",
    "2. Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n",
    "3. Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing.\n",
    "    - Compute the sum of the squared distance between data points and all centroids.\n",
    "    - Assign each data point to the closest cluster (centroid).\n",
    "    - Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach kmeans follows to solve the problem is called Expectation-Maximization. \n",
    "1. The E-step is assigning the data points to the closest cluster. \n",
    "2. The M-step is computing the centroid of each cluster.\n",
    "\n",
    "K-means itself is NP-HARD. But there are approximation algorithms that converge to a local minimum, one of which is the Lloyd's algorithm which we will use for this section of the project; though we will implement the k-means++ with a smart initialization of all the centroids as would be seen in later sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lloyd's Algorithm for k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mu_1, \\cdots, \\mu_k \\leftarrow$ randomly chosen centers\n",
    "- **while** Objective function still improves **do** \n",
    "    - $S_i, \\cdots, S_k \\leftarrow \\phi$\n",
    "    - **for** i $\\in$ 1,..., N **do**\n",
    "        - j $\\leftarrow \\text{argmin}_j ||x_i - \\mu_j||^2$\n",
    "        - add i to $S_j$\n",
    "    - **end for**\n",
    "    - **for** j $\\in$ 1,...,k **do**\n",
    "        - $\\mu_j = \\frac{1}{|S_j|} \\sum_{i \\in S_j} x_i$\n",
    "    - **end for**\n",
    "- **end while**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means++\n",
    "\n",
    "This algorithm ensures a smarter initialization of the centroids and \"improves\" the quality of the clustering. (Really? We shall elaborate on this later.) Apart from initialization, the rest of the algorithm is the same as the standard K-means algorithm. That is K-means++ is the standard K-means algorithm coupled with a smarter initialization of the centroids.\n",
    " \n",
    "Still sensitive to outliers as we shall explain in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialisation**\n",
    "\n",
    "- C $\\leftarrow \\text{sample a point uniformly from } \\mathbf{X} $\n",
    "- **while** |C| < k **do**\n",
    "    - sample x $\\in \\mathbf{X} \\text{ with probability proportional to } d^2 (x,C), \\\\\n",
    "     \\text{where d(x,C) is the distance between x and centroid C }$\n",
    "    - C $\\leftarrow \\cup \\space{} \\{x\\}$\n",
    "- **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm in plain english\n",
    "\n",
    "1. Choose one center uniformly at random among the data points.\n",
    "2. For each data point x not chosen yet, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
    "3. Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to $D(x)^2$.\n",
    "4. Repeat Steps 2 and 3 until k centers have been chosen.\n",
    "5. Now that the initial centers have been chosen, proceed using standard k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class implementation of the k-means++ algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans_plus_plus:\n",
    "    def __init__(self, K=4, init=\"kmeans++\", rand_seed=None):\n",
    "        self.K = K\n",
    "        self.init = init\n",
    "        self.rand_seed = rand_seed\n",
    "    \n",
    "        # list of sample indices for each cluster\n",
    "        self.clusters = [[] for _ in range(self.K)]\n",
    "        # the centers (mean feature vector) for each cluster\n",
    "        self.centroids = []\n",
    "\n",
    "    ##########################################################################################\n",
    "    # initialization algorithm\n",
    "    def __initialize_centroids_further_first(self):\n",
    "        \n",
    "        centroids = list( self.__random_centroid(how_many_centroids=1) )\n",
    "    \n",
    "        ## compute the remaining k - 1 centroids\n",
    "        for _ in range(self.K - 1):\n",
    "            next_cent_id = self.__max_arg(centroids)\n",
    "            centroids.append(self.X[next_cent_id, :])\n",
    "        return np.array(centroids)\n",
    "\n",
    "    \n",
    "    def __plus_plus_init(self):\n",
    "        np.random.seed(self.rand_seed)\n",
    "        # (K, d) \n",
    "        centroids = np.zeros((self.K, self.X.shape[1]), dtype=np.float32)\n",
    "        # First centroid | note randint sample from discrete uniform\n",
    "        centroids[0,:] = self.X[np.random.randint(low = 0, high = self.X.shape[0], size = 1), : ]\n",
    "\n",
    "        for k in range (1, self.K) : \n",
    "            # Calculate the distance to the closest centroid for each point\n",
    "            d = np.asarray([self.__min_dist(point, centroids[:k,:]) for point in self.X])\n",
    "            d_sum = np.sum(d)\n",
    "\n",
    "            # Turn the distance into probabilities that sums up to one\n",
    "            d /= d_sum\n",
    "        \n",
    "            # sample with distance as weight the new centroid\n",
    "            centroids[k, :] = self.X[np.random.choice(self.X.shape[0], p=d),:]\n",
    "\n",
    "        return centroids         \n",
    "    \n",
    "    def __max_arg(self, centroids):\n",
    "        return np.argmax([ self.__min_dist(point,centroids ) for point in self.X])\n",
    "        \n",
    "    def __min_dist(self, point, centroids):\n",
    "        return min( [ self.euclidean_distance(point, centroid) for centroid in centroids] ) \n",
    "    \n",
    "    ##########################################################################################\n",
    "    # get the how_many_centroids \"centroids\". 1 if k-means++, but k if k-means\n",
    "    def __random_centroid(self, how_many_centroids):\n",
    "        self.__set_seed() \n",
    "        l = self.X.shape[0]\n",
    "        choices = np.random.choice(l, how_many_centroids)\n",
    "        return np.array(self.X[choices, :])\n",
    "    \n",
    "    # set the random seed for reproducibility\n",
    "    def __set_seed(self):\n",
    "        if self.rand_seed is not None and isinstance(self.rand_seed, int):\n",
    "            np.random.seed(self.rand_seed)\n",
    "    ##########################################################################################\n",
    "    def fit_predict(self, X):\n",
    "        self.X = X\n",
    "        # print(self.X.shape)\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "\n",
    "        if self.init == 'further_first': \n",
    "            self.centroids = self.__initialize_centroids_further_first()\n",
    "        elif self.init is None: \n",
    "            self.centroids = self.__random_centroid(how_many_centroids=self.K)\n",
    "        # Kmean ++\n",
    "        else: \n",
    "            self.centroids = self.__plus_plus_init()\n",
    "\n",
    "        \n",
    "                    \n",
    "        #Initialize an empty array to compare with current centroids to see if convergence occurs...\n",
    "        i, j = self.centroids.shape\n",
    "        centroids_old = np.zeros(shape=(i, j) )\n",
    "\n",
    "        #if the current centroids are the same as the previous centroids, then stop\n",
    "        while not self.__has_converged(centroids_old, self.centroids) :\n",
    "            \n",
    "            # Assign samples to closest centroids (create clusters)\n",
    "            self.clusters = self.__create_clusters(self.centroids)\n",
    "\n",
    "            # Calculate new centroids from the clusters\n",
    "            centroids_old = self.centroids\n",
    "            self.centroids = self.__centroids_means_of_clusters(self.clusters)\n",
    "\n",
    "        # Classify samples as the index of their clusters\n",
    "        return self.__get_cluster_labels(self.clusters)\n",
    "\n",
    "    # each sample will get the label of the cluster it was assigned to\n",
    "    def __get_cluster_labels(self, clusters):\n",
    "        labels = np.empty(self.n_samples)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            labels[cluster] = cluster_idx\n",
    "        return labels\n",
    "    \n",
    "    # Assign the samples to the closest centroids to create clusters; Expectation step\n",
    "    def __create_clusters(self, centroids):\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for idx, sample in enumerate(self.X):\n",
    "            centroid_idx = self.__closest_centroid_label(sample, centroids)\n",
    "            clusters[centroid_idx].append(idx)\n",
    "        return clusters\n",
    "    \n",
    "    # arg that minimizes the distance of the current sample to each centroid\n",
    "    def __closest_centroid_label(self, point, centroids):\n",
    "        return np.argmin( [ self.euclidean_distance(point, centroid) for centroid in centroids] ) \n",
    "\n",
    "    # assign mean value of clusters to centroids; Maximisation step\n",
    "    def __centroids_means_of_clusters(self, clusters):\n",
    "        return [ np.mean(self.X[cluster], axis=0) for cluster in clusters]\n",
    "        \n",
    "    # distances between each old and new centroids\n",
    "    def __has_converged(self, old_centroids, current_centroids):\n",
    "        distances = [self.euclidean_distance(old_centroids[i], current_centroids[i]) for i in range(self.K)]\n",
    "        return sum(distances) == 0\n",
    "    \n",
    "    def euclidean_distance(self, point1, point2):\n",
    "        dist = la.norm(point1 - point2)\n",
    "        return np.square(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 3 3 3] [3 3 3 ... 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ModeResult(mode=array([3], dtype=uint8), count=array([222])),\n",
       " 1: ModeResult(mode=array([0], dtype=uint8), count=array([297])),\n",
       " 2: ModeResult(mode=array([2], dtype=uint8), count=array([399])),\n",
       " 3: ModeResult(mode=array([1], dtype=uint8), count=array([270]))}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Our own implementation\n",
    "\n",
    "#k = KMeans_plus_plus(K=4, init=\"k-means\", rand_seed=4)\n",
    "k = KMeans_plus_plus(K=4, init=\"kmeans++\", rand_seed=4)   \n",
    "y_pred = k.fit_predict(X_standardised.values).astype(np.uint8)\n",
    "\n",
    "from scipy import stats\n",
    "#confusion_matrix_score(y, y_pred)\n",
    "print(y,y_pred)\n",
    "dic = {0:[], 1:[], 2:[], 3:[]}\n",
    "for i in range(len(y)):\n",
    "    dic[y[i]].append(y_pred[i])\n",
    "for n in range(4):\n",
    "    dic[n] = stats.mode(dic[n])\n",
    "\n",
    "dic\n",
    "\n",
    "#print(accuracy_score(y, y_pred))\n",
    "\n",
    "# for i in list(zip(y, y_pred)):\n",
    "#     print(i, end=\"  \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is 21.71%.\n",
      "\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Classic       0.00      0.00      0.00       244\n",
      "        Jazz       0.90      0.77      0.83       382\n",
      "       Metal       0.01      0.01      0.01       406\n",
      "         Rap       0.00      0.00      0.00       350\n",
      "\n",
      "    accuracy                           0.22      1382\n",
      "   macro avg       0.23      0.20      0.21      1382\n",
      "weighted avg       0.25      0.22      0.23      1382\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# sk-learn\n",
    "km = KMeans(n_clusters=4, init=\"k-means++\")\n",
    "\n",
    "y_pred_2 = km.fit_predict(X_standardised)\n",
    "\n",
    "confusion_matrix_score(y, y_pred_2)\n",
    "\n",
    "# for i in list(zip(y, y_pred_2)):\n",
    "#     print(i, end=\"  \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how can one measure clustering goodness of fit? Supervised algorithms have lots of metrics to check their goodness of fit like accuracy, r-square value, sensitivity, specificity etc. but what can we calculate to measure the accuracy or goodness of our clustering technique? The answer to this question is Silhouette Coefficient or Silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the silhouette score to check the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Silhouette Coefficient**](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py)  \n",
    "Silhouette score for a set of sample data points is used to measure how dense and well-separated the clusters are.  \n",
    "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (sklearn.metrics.silhouette_score) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:  \n",
    "- a: The mean distance between a sample and all other points in the same class.\n",
    "- b: The mean distance between a sample and all other points in the next nearest cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette Coefficient, s, for a single sample is then given as:\n",
    "\n",
    "$$s = \\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "- The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.\n",
    "- The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawbacks**\n",
    "- The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 is closest to label 3\n",
      "Label 1 is closest to label 2\n",
      "Label 2 is closest to label 0\n",
      "Label 3 is closest to label 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3277068051280422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def silhouette_score(X, y, centroids):\n",
    "    list_1 = []\n",
    "    closest_cents = nearest_cluster(centroids, centroids)\n",
    "    \n",
    "    #for each cluster and its closest cluster, do\n",
    "    for i, val in enumerate(closest_cents):\n",
    "        print(f\"Label {i} is closest to label {val}\")\n",
    "        \n",
    "        x    = X[y==i, :]\n",
    "        cent = X[y==val, :]\n",
    "        \n",
    "        d = sample_score(x, cent)\n",
    "        list_1.extend(d)\n",
    "                \n",
    "    l = X.shape[0]\n",
    "    return sum(list_1)/l\n",
    "    \n",
    "\n",
    "def nearest_cluster(centroids1, centroids2):\n",
    "    closest_cents = []\n",
    "    for val1 in centroids1 :\n",
    "        list_dist = [k.euclidean_distance(val1, val2) for val2 in centroids2]\n",
    "        \n",
    "        #dist btw a cent and itself is 0 (minimum), but we are interested in min dist btw different centroids\n",
    "        #That is, we are interested in the second min dist\n",
    "        second_min_dist = np.argsort(list_dist)[1] #sorted(range(len(list_dist)), key=lambda i: list_dist[i])[1]\n",
    "        \n",
    "        closest_cents.append(second_min_dist)\n",
    "        \n",
    "    return closest_cents\n",
    "    \n",
    "#Array of distances btw data points\n",
    "def sample_score(X1, X2):\n",
    "    \n",
    "    l1 = X1.shape[0] - 1  #-1 for excluding the data point itself\n",
    "    l2 = X2.shape[0]\n",
    "    list_sum = []\n",
    "    \n",
    "    for val1 in X1:\n",
    "        a = get_dist(val1, X1)/l1 #for any other in-cluster data point excluding val1 itself...\n",
    "        b = get_dist(val1, X2)/l2 #for all data points in the closest cluster...\n",
    "        m = max(a, b)\n",
    "        \n",
    "        s = (b-a)/m\n",
    "        list_sum.append(s)\n",
    "    return list_sum\n",
    "\n",
    "#Dist btw a single data point and all the rest\n",
    "def get_dist(val1, X2):\n",
    "    sum_for_single_data_point = sum( [k.euclidean_distance(val1, val2) for val2 in X2] )\n",
    "    return sum_for_single_data_point\n",
    "    \n",
    "    \n",
    "silh_score = silhouette_score(X_standardised.values, y_pred, k.centroids)\n",
    "silh_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sk-learn silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17609809999313508"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score as sls\n",
    "\n",
    "sls(X = X_standardised.values, metric=\"euclidean\", labels=y_pred_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One may wonder whether or not kmeans++ always does a better job in clustering than kmeans. The answer is that kmeans++ does not always outperform kmeans. A plot would help to better understand. So let's see some plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x1 = np.random.uniform(low=1, high=10, size=50)\n",
    "y1 = np.random.uniform(low=1, high=3, size=50)\n",
    "\n",
    "x2 = np.random.uniform(low=15, high=25, size=50)\n",
    "y2 = np.random.uniform(low=6, high=9, size=50)\n",
    "\n",
    "x_outlier = np.random.uniform(low=-20,high=-3, size=2)\n",
    "y_outlier = np.random.uniform(low=-30,high=-3, size=2)\n",
    "\n",
    "xs = np.concatenate([x1, x2, x_outlier], axis=0)\n",
    "ys = np.concatenate([y1, y2, y_outlier], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.939322</td>\n",
       "      <td>2.140394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.436704</td>\n",
       "      <td>1.877203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.424870</td>\n",
       "      <td>2.976748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.903949</td>\n",
       "      <td>1.204090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.812893</td>\n",
       "      <td>1.417754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.813047</td>\n",
       "      <td>1.322619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.938285</td>\n",
       "      <td>2.306217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.025957</td>\n",
       "      <td>1.506583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.672965</td>\n",
       "      <td>1.932622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.450974</td>\n",
       "      <td>1.488851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2\n",
       "0  5.939322  2.140394\n",
       "1  7.436704  1.877203\n",
       "2  6.424870  2.976748\n",
       "3  5.903949  1.204090\n",
       "4  4.812893  1.417754\n",
       "5  6.813047  1.322619\n",
       "6  4.938285  2.306217\n",
       "7  9.025957  1.506583\n",
       "8  9.672965  1.932622\n",
       "9  4.450974  1.488851"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.DataFrame(list(zip(xs, ys)), columns = [\"X1\", \"X2\"])\n",
    "dat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#change the init=\"kmeans++\" to any other string to see how kmeans bits kmeans++ for this toy example\n",
    "def testing_inits(k=2, init=\"kmeans++\", rand_seed=4):\n",
    "    k_toy = KMeans_plus_plus(K=k, init=init, rand_seed=rand_seed) #set k = 2, 3 and seed = 4 and 0 to see...\n",
    "\n",
    "    pred_k_toy = k_toy.fit_predict(dat.values)\n",
    "\n",
    "    k_toy.centroids = np.array(k_toy.centroids)\n",
    "\n",
    "    if not init == \"kmeans++\": init = \"kmeans\"\n",
    "    \n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.scatter(dat[\"X1\"], dat[\"X2\"], c=pred_k_toy)\n",
    "    \n",
    "    plt.scatter(k_toy.centroids[:, 0], k_toy.centroids[:, 1], marker = \"o\", linewidths=5, c=\"red\")\n",
    "    plt.title(init)\n",
    "    plt.show()\n",
    "\n",
    "#pred_k_toy, k_toy.centroids#, k_toy.clusters\n",
    "\n",
    "k_p = testing_inits(k=2, init=\"kmeans++\", rand_seed=4)\n",
    "k1 = testing_inits(k=2, init=\"kmeans\", rand_seed=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this toy example, when k=2, kmeans clusters the data better. However, for other k values, either kmeans++ outperforms it or they are on a par with each other. Therefore, We can conclude that kmeans++ is sensitive to outliers and does not always outperform kmeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "379ef13cdaaa3488d0d0ee4a3d00b9f61dd7f8c251b8a40c38fb20881b24658a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
